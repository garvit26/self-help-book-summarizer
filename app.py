# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1siC_P1GV64hYTof5T6zu1QmaXHaEd7oo
"""

# app.py
# app.py
# app.py
import streamlit as st
import fitz  # PyMuPDF
from langchain_core.documents.base import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma  # ‚Üê NEW FIX FOR CHROMA
from langchain_core.prompts import PromptTemplate
import os
import tempfile

# Page config
st.set_page_config(page_title="Self-Help Book Summarizer", layout="wide")
st.title("üìö Self-Help Book Chapter-Wise Summarizer")
st.markdown("Upload your self-help PDFs and get structured chapter summaries powered by local AI.")

# Initialize session state
if "chapters" not in st.session_state:
    st.session_state.chapters = []
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "processed_books" not in st.session_state:
    st.session_state.processed_books = []

# Sidebar
with st.sidebar:
    st.header("Settings")
    model_name = st.selectbox(
        "LLM Model for Summaries",
        options=["llama3.1:8b", "phi3", "gemma2:9b", "mistral"],
        index=0
    )
    embed_model = st.selectbox(
        "Embedding Model",
        options=["nomic-embed-text"],
        help="Used for searching chapters"
    )
    st.info("Make sure Ollama is running: `ollama serve`")

# Function to extract chapters
def extract_chapters(pdf_file, book_name):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_path = tmp_file.name

    doc = fitz.open(tmp_path)
    toc = doc.get_toc()

    # Try TOC first (best case)
    if toc:
        # Same as before ‚Äî your original TOC logic
        chapters = []
        for i in range(len(toc)):
            lvl, title, start_page = toc[i]
            if lvl != 1: continue
            title = title.strip().replace("\n", " ")
            end_page = toc[i+1][2] - 1 if i+1 < len(toc) and toc[i+1][0] == 1 else doc.page_count

            text = ""
            for page_num in range(start_page-1, end_page):
                page = doc.load_page(page_num)
                text += page.get_text("text") + "\n"

            chapters.append(Document(
                page_content=text.strip(),
                metadata={"book_title": book_name, "chapter_title": title,
                          "start_page": start_page, "end_page": end_page}
            ))
        doc.close()
        os.unlink(tmp_path)
        return chapters

    # NEW: Fallback heuristic chapter detection
    st.warning(f"No TOC found in {book_name}. Using smart heading detection...")
    chapters = []
    current_text = ""
    current_title = "Introduction"
    current_start = 1

    import re
    chapter_pattern = re.compile(
        r'^(chapter\s*\d+|^\d+\.?|^\s*[IVXLCDM]+\.?\s)', 
        re.IGNORECASE | re.MULTILINE
    )
    heading_pattern = re.compile(
        r'^([A-Z][A-Z\s]{10,}|^\d+\.\s+[A-Z][A-Za-z\s]+|Chapter\s+\d+[\s:]+[A-Za-z\s]+)',
        re.MULTILINE
    )

    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text = page.get_text("text")
        lines = text.split('\n')

        for line in lines:
            line_stripped = line.strip()
            if chapter_pattern.match(line_stripped) or heading_pattern.match(line_stripped):
                # Save previous chapter if exists
                if current_text.strip():
                    chapters.append(Document(
                        page_content=current_text.strip(),
                        metadata={
                            "book_title": book_name,
                            "chapter_title": current_title,
                            "start_page": current_start,
                            "end_page": page_num + 1
                        }
                    ))
                # Start new chapter
                current_title = line_stripped[:100]  # Truncate long titles
                current_text = line + "\n"
                current_start = page_num + 2
            else:
                current_text += line + "\n"

    # Add final chapter
    if current_text.strip():
        chapters.append(Document(
            page_content=current_text.strip(),
            metadata={
                "book_title": book_name,
                "chapter_title": current_title or "Conclusion",
                "start_page": current_start,
                "end_page": doc.page_count
            }
        ))

    # Final fallback: if no chapters detected, split by page count
    if len(chapters) <= 1:
        st.info("Few headings found. Splitting book into ~50-page sections.")
        pages_per_chapter = 50
        full_text = "\n".join(page.get_text("text") for page in doc)
        words = full_text.split()
        chunk_size = len(words) // max(1, (len(words) // (pages_per_chapter * 400)))  # ~400 words/page estimate

        for i in range(0, len(words), chunk_size * 10):  # Larger chunks
            chunk_words = words[i:i + chunk_size * 10]
            chunk_text = " ".join(chunk_words)
            chap_num = i // (chunk_size * 10) + 1
            chapters.append(Document(
                page_content=chunk_text,
                metadata={
                    "book_title": book_name,
                    "chapter_title": f"Section {chap_num}",
                    "start_page": 1 + (i // 400),
                    "end_page": min(doc.page_count, 1 + ((i + chunk_size * 10) // 400))
                }
            ))

    doc.close()
    os.unlink(tmp_path)
    return chapters
# Upload PDFs
st.header("Upload Books")
uploaded_files = st.file_uploader(
    "Choose PDF files",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    for uploaded_file in uploaded_files:
        book_name = uploaded_file.name.replace(".pdf", "")
        if book_name in st.session_state.processed_books:
            continue

        with st.spinner(f"Processing {uploaded_file.name}..."):
            new_chapters = extract_chapters(uploaded_file, book_name)
            st.session_state.chapters.extend(new_chapters)
            st.session_state.processed_books.append(book_name)
            st.success(f"‚úÖ Extracted {len(new_chapters)} chapters from {uploaded_file.name}")

# Build vector database
if st.session_state.chapters and (st.session_state.vectorstore is None or st.button("Rebuild Search Index")):
    with st.spinner("Building search index (this may take a minute)..."):
        embeddings = OllamaEmbeddings(model=embed_model)
        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        split_docs = splitter.split_documents(st.session_state.chapters)

        st.session_state.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
    st.success("üîç Search index ready!")
# Add this new block right after the index success message
if st.session_state.chapters:
    st.header("Full Book Summary")
    
    if st.button("Generate Summary of the Entire Book"):
        with st.spinner("Generating full book summary... (this may take a few minutes)"):
            # Combine all chapter contents (but limit to avoid token overload)
            full_text = ""
            for chap in st.session_state.chapters:
                full_text += f"\n\n=== {chap.metadata['chapter_title']} ===\n" + chap.page_content + "\n"
            
            # Safety: limit total text length (phi3 can handle ~8k-16k tokens)
            max_chars = 80000  # ~20k tokens approx
            full_text = full_text[:max_chars] + "... (truncated for model capacity)"

            llm = ChatOllama(model=model_name, temperature=0.4)  # Slightly higher temp for creative summary

            prompt = PromptTemplate.from_template("""
            You are an expert self-help book summarizer. Provide a comprehensive, insightful summary of the entire book below.

            Focus on:
            - Overall main theme and core message
            - Key parts or sections
            - Most important concepts, frameworks, and principles
            - Practical takeaways and action steps
            - Any memorable quotes or stories

            Book Title: {book_title}

            Book Content (may be truncated):
            {full_content}

            Full Book Summary:
            """)

            chain = prompt | llm
            result = chain.invoke({
                "book_title": st.session_state.chapters[0].metadata["book_title"],
                "full_content": full_text
            })

            st.markdown("### üìñ Full Book Summary")
            st.write(result.content)

            # Optional: Download button
            st.download_button(
                label="Download Full Book Summary",
                data=result.content,
                file_name=f"{st.session_state.chapters[0].metadata['book_title']}_full_summary.txt",
                mime="text/plain"
            )
# Display chapters and summarize
if st.session_state.chapters:
    st.header("Chapters")
    for i, chap in enumerate(st.session_state.chapters):
        with st.expander(f"**{chap.metadata['book_title']}** ‚Äî {chap.metadata['chapter_title']} (Pages {chap.metadata['start_page']}-{chap.metadata['end_page']})"):
            if st.button("Generate Summary", key=f"sum_{i}"):
                with st.spinner("Generating summary..."):
                    llm = ChatOllama(model=model_name, temperature=0.3)

                    prompt = PromptTemplate.from_template("""
                    You are an expert self-help book summarizer. Create a clear, insightful summary of the chapter below.

                    Focus on:
                    - Main theme and core message
                    - Key concepts, frameworks, or models
                    - Practical advice and action steps
                    - Memorable quotes or stories

                    Chapter: {chapter_title}
                    From: {book_title}

                    Content:
                    {context}

                    Summary:
                    """)

                    chain = prompt | llm
                    result = chain.invoke({
                        "chapter_title": chap.metadata["chapter_title"],
                        "book_title": chap.metadata["book_title"],
                        "context": chap.page_content
                    })
                    st.markdown("### üìù Summary")
                    st.write(result.content)
                    # Optional: Simple retrieval quality metric (for resume/portfolio value)
                    with st.expander("üîç Retrieval Quality (Technical Insight)"):
    # Dummy score for now (since we're using full chapter as context)
    # In a full RAG setup, you'd compute cosine similarity between query embedding and retrieved chunks
                      st.metric(
                       label="Context Relevance Score",
                       value="0.98",
                       delta="+ High confidence",
                       help="Simulated score showing how well the retrieved text matches the chapter. In production RAG, this uses cosine similarity from embeddings."
                      )
                      st.caption("This is a placeholder metric to demonstrate evaluation capability. Real implementation would compare query vs retrieved chunk embeddings.")
else:
    st.info("Upload one or more PDF books to get started!")




st.markdown("---")
st.caption("Built with ‚ù§Ô∏è using Streamlit + LangChain + Ollama (100% local & private)")